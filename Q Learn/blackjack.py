# -*- coding: utf-8 -*-
"""blackjack.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UWkJ4bVc2_exF3_e9QhtdIzt-7HYmOeG
"""

from typing import Tuple
import numpy as np
import numpy.typing as npt
import matplotlib.pyplot as plt

"""
WRITE THIS FUNCTION
"""
def value_iteration(
    V0: npt.NDArray, 
    lr: float, 
    gamma:float, 
    epsilon: float=1e-12
    ) -> npt.NDArray:
  V = V0
  change = epsilon+1
  
  while epsilon < change:
    m= 0#maximum change in V
    V_next = []
    for state in range(22):
      total = 0
      for i in range(1,10):
        x = state+i
        if x > 21:
          continue
        total += gamma*V[x]
      x = state+10
      if x <= 21:
        total += 4*gamma*V[x]
      total /= 13
      total+=lr
      val = max(state,total)
      m = max(m,abs(val-V[state]))
      V_next.append(val)
    V = np.array(V_next)
    change = m

  return V

"""
WRITE THIS FUNCTION
"""
def value_to_policy(V: npt.NDArray, lr: float, gamma: float) -> npt.NDArray:
  policy = []
  for i in range(len(V)):
    if i >= V[i]:
      policy.append(0)
    else:
      policy.append(1)
  
  return np.array(policy)

lr = -4
gamma = 1
V = value_iteration(np.zeros(22),lr,gamma)
P = value_to_policy(V,lr,gamma)
print(V)
print(P)

plt.figure()
plt.plot(V)
plt.title("V*")

plt.figure()
plt.plot(P)
plt.title("pi*")

from numpy.lib.function_base import kaiser
def draw() -> int:
  probs = 1/13*np.ones(10)
  probs[-1] *= 4
  return np.random.choice(np.arange(1,11), p=probs)


"""
WRITE THIS FUNCTION
"""
def Qlearn(
    Q0: npt.NDArray, 
    lr: float, 
    gamma: float, 
    alpha: float, 
    epsilon: float, 
    N: int
    ) -> Tuple[npt.NDArray, npt.NDArray]:
  Q = Q0
  record = np.zeros((N,3))
  s = 0
  for i in range(N):
    action = 0
    if Q[s][1]> Q[s][0]:
      action = 1
    if random.random() > 1-epsilon + (epsilon/2):#explore
      action = action^1
    s2 = 0
    if action == 0:
      reward = s
      Q[s][action] = Q[s][action] + alpha*(reward-Q[s][action])
    else:
      reward = lr
      s2 = s+draw()
      if s2>21:
        s2 = 0
        Q[s][action] = Q[s][action] + alpha*(reward-Q[s][action])
      else:
        Q[s][action] = Q[s][action] + alpha*(reward+ gamma*max(Q[s2][0],Q[s2][1])-Q[s][action])
    record[i][0] = s
    record[i][1] = action
    record[i][2] = reward
    s = s2


  return Q, record

import random

lr = 0
gamma = 1
alpha = 0.1
epsilon = 0.1
N = 50000
Q,record = Qlearn(np.zeros((22,2)),lr,gamma,alpha,epsilon,N)
print(record)

def RL_analysis():
  lr, gamma, alpha, epsilon, N = 0, 1, 0.1, 0.1, 10000
  visits = np.zeros((22,6))
  rewards = np.zeros((N,6))
  values = np.zeros((22,6))

  for i in range(6):
    _, record = Qlearn(np.zeros((22,2)), lr, gamma, alpha, epsilon, 10000*i)
    vals, counts = np.unique(record[:,0], return_counts=True)
    visits[vals.astype(int),i] = counts
    _, record = Qlearn(np.zeros((22,2)), lr, gamma, alpha, 0.2*i, N)
    rewards[:,i] = record[:,2]
    vals, _ = Qlearn(np.zeros((22,2)), lr, gamma, min(0.2*i+0.1,1), epsilon, N)
    values[:,i] = np.max(vals, axis=1)

  plt.figure()
  plt.plot(visits)
  plt.legend(['N=0', 'N=10k', 'N=20k', 'N=30k' ,'N=40k', 'N=50k'])
  plt.title('Number of visits to each state')

  plt.figure()
  plt.plot(np.cumsum(rewards, axis=0))
  plt.legend(['e=0.0', 'e=0.2', 'e=0.4' ,'e=0.6', 'e=0.8', 'e=1.0'])
  plt.title('Cumulative rewards received')

  plt.figure()
  plt.plot(values)
  plt.legend(['a=0.1' ,'a=0.3', 'a=0.5', 'a=0.7', 'a=0.9', 'a=1.0'])
  plt.title('Estimated state values');

RL_analysis()